[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.AnalysisException: cannot resolve 'total_amount' given input columns: [total_amount.account_id, total_amount.sum(transaction_amount)];[0m
[0m[[0m[31merror[0m] [0m[0m'Sort ['total_amount DESC NULLS LAST], true[0m
[0m[[0m[31merror[0m] [0m[0m+- SubqueryAlias total_amount[0m
[0m[[0m[31merror[0m] [0m[0m   +- Aggregate [account_id#9], [account_id#9, sum(transaction_amount#10) AS sum(transaction_amount)#60L][0m
[0m[[0m[31merror[0m] [0m[0m      +- Project [account_id#9, transaction_amount#10][0m
[0m[[0m[31merror[0m] [0m[0m         +- Project [transaction_id#8, account_id#9, transaction_amount#10, to_date('transaction_date, Some(yyyy-MM-dd)) AS transaction_date#33][0m
[0m[[0m[31merror[0m] [0m[0m            +- Project [_1#0 AS transaction_id#8, _2#1 AS account_id#9, _3#2 AS transaction_amount#10, _4#3 AS transaction_date#11][0m
[0m[[0m[31merror[0m] [0m[0m               +- LocalRelation [_1#0, _2#1, _3#2, _4#3][0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:179)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:175)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:535)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:535)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:532)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.map(List.scala:246)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.map(List.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:532)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:181)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:181)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:161)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:175)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:172)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:195)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:192)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:211)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3739)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:3727)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.sort(Dataset.scala:1288)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.orderBy(Dataset.scala:1309)[0m
[0m[[0m[31merror[0m] [0m[0m	at Practice$.delayedEndpoint$Practice$1(practice.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at Practice$delayedInit$body.apply(practice.scala:4)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Function0.apply$mcV$sp(Function0.scala:42)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Function0.apply$mcV$sp$(Function0.scala:42)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.$anonfun$main$1(App.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.$anonfun$main$1$adapted(App.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterable.foreach(Iterable.scala:933)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.main(App.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.main$(App.scala:96)[0m
[0m[[0m[31merror[0m] [0m[0m	at Practice$.main(practice.scala:4)[0m
[0m[[0m[31merror[0m] [0m[0m	at Practice.main(practice.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:144)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:187)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunMainTask$7(Defaults.scala:1998)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1969)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:367)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:834)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrunMain[0m) org.apache.spark.sql.AnalysisException: cannot resolve 'total_amount' given input columns: [total_amount.account_id, total_amount.sum(transaction_amount)];[0m
[0m[[0m[31merror[0m] [0m[0m'Sort ['total_amount DESC NULLS LAST], true[0m
[0m[[0m[31merror[0m] [0m[0m+- SubqueryAlias total_amount[0m
[0m[[0m[31merror[0m] [0m[0m   +- Aggregate [account_id#9], [account_id#9, sum(transaction_amount#10) AS sum(transaction_amount)#60L][0m
[0m[[0m[31merror[0m] [0m[0m      +- Project [account_id#9, transaction_amount#10][0m
[0m[[0m[31merror[0m] [0m[0m         +- Project [transaction_id#8, account_id#9, transaction_amount#10, to_date('transaction_date, Some(yyyy-MM-dd)) AS transaction_date#33][0m
[0m[[0m[31merror[0m] [0m[0m            +- Project [_1#0 AS transaction_id#8, _2#1 AS account_id#9, _3#2 AS transaction_amount#10, _4#3 AS transaction_date#11][0m
[0m[[0m[31merror[0m] [0m[0m               +- LocalRelation [_1#0, _2#1, _3#2, _4#3][0m
